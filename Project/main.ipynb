{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2PqFX1NImtk",
        "outputId": "74964749-d21f-4165-ebc9-68f57f399a15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The total time taken to run the entire program including Tokenization and Indexing is 0.12681221961975098 seconds\n"
          ]
        }
      ],
      "source": [
        "# Importing Regular Expression and Porter Stemmer libraries\n",
        "import re\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from operator import itemgetter\n",
        "from collections import defaultdict\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# This is text parser class that I created to handle all parsing functions\n",
        "class TextParser_Indexer:\n",
        "    def __init__(self, stop_words=None):\n",
        "        # first we intitialize stop words using Tokenizer class\n",
        "        self.tokenizer = Tokenizer(stop_words)\n",
        "        # We create instances for Word Dictionary class\n",
        "        self.word_dict = WordDictionary()\n",
        "        # We create instances for file Dictionary class\n",
        "        self.file_dict = FileDictionary()\n",
        "        # We create instances for forward and inverted index class\n",
        "        self.indexer = Indexer()\n",
        "\n",
        "        self.time_taken = 0\n",
        "\n",
        "    # This is the function that loads the file content and \n",
        "    def parse_file(self, file_path, forward_index_file):\n",
        "        with open(file_path, 'r') as file:\n",
        "            file_content = file.read()\n",
        "            # This is the regular expression pattern to find and iterate over matches of the pattern \"<DOC>...</DOC>\" \n",
        "            doc_tag_matches = re.finditer(r'<DOC>(.*?)<\\/DOC>', file_content, re.DOTALL)\n",
        "            # This loop iterates over each document in the file\n",
        "            for doc_tag_match in doc_tag_matches:\n",
        "                # get the content between \"<DOC>...</DOC>\" \n",
        "                document_content = doc_tag_match.group(1)\n",
        "                # Call function parse_document to parse through each document\n",
        "                self.parse_document(document_content, forward_index_file)\n",
        "    \n",
        "    # This function parses each document within the files\n",
        "    def parse_document(self, document, forward_index_file):\n",
        "        # Now within the content we have try to find the '<DOCNO>...</DOCNO>' and '<TEXT>...</TEXT>'\n",
        "        doc_text_match = re.search(r'<DOCNO>(.*?)</DOCNO>.*?<TEXT>(.*?)</TEXT>', document, re.DOTALL)\n",
        "        if not doc_text_match:\n",
        "            return\n",
        "\n",
        "        # doc_tag_match.group(1) has the content between '<DOCNO>...</DOCNO>' tags\n",
        "        file_doc_id = self.file_dict.map_document_to_id(doc_text_match.group(1))  \n",
        "\n",
        "        # doc_tag_match.group(2) has the content between '<TEXT>...</TEXT>'\n",
        "        text_content = doc_text_match.group(2)\n",
        "\n",
        "        # This function tokenizes the document content into lower case tokens and removes numeric, splits on non-alphanumeric and eliminates stop words. \n",
        "        token_stream = self.tokenizer.tokenize(text_content)\n",
        "\n",
        "        # Now we stem the tokenized words to their root words\n",
        "        token_stemmed = [self.word_dict.stem(token) for token in token_stream]\n",
        "\n",
        "        # print(f\"\\n{file_doc_id}\\t{', '.join(map(str, token_stemmed))}\")\n",
        "\n",
        "        # Here the output is stored in dictionaries, we save both the file name with unique file ID and stemmed token/word with their unique ID\n",
        "        for _, token_stem in zip(token_stream, token_stemmed):\n",
        "            word_token_id = self.word_dict.map_word_to_id(token_stem)\n",
        "            self.word_dict.map_word_id_to_doc(word_token_id, file_doc_id)\n",
        "        \n",
        "        # Below function uses word_dict and file_dict to create forward and inverted index\n",
        "        start_time = time.time()\n",
        "        self.indexer.index_building(file_doc_id, token_stemmed, self.word_dict.word_map_id )\n",
        "        end_time = time.time()\n",
        "\n",
        "        self.time_taken = self.time_taken + (end_time - start_time)\n",
        "\n",
        "\n",
        "    # This function simply saves the file and word dictionaries \n",
        "    def save_dictionary(self, output_file, inverted_index_file, forward_index_file):\n",
        "\n",
        "        word = self.word_dict.word_map_id\n",
        "        file = self.file_dict.doc_map_id\n",
        "\n",
        "        combined_dict = {**word, **file}\n",
        "        # we also combine the word and file dictionaries together and save it in output file parser_output.txt\n",
        "        with open(output_file, 'w') as output_file:\n",
        "            for key, value in combined_dict.items():\n",
        "                output_file.write(f\"{key}\\t{value}\\n\")\n",
        "\n",
        "        \n",
        "        with open(inverted_index_file, 'w') as output_file:\n",
        "            for word, documents in self.indexer.inverted_index.items():\n",
        "                formatted_list =[]\n",
        "                for doc_id, freq in documents.items():\n",
        "                    formatted_list.append(f\"{doc_id}: {freq};\")\n",
        "                output_file.write(f\"{word}\\t{' '.join(formatted_list)}\\n\")  \n",
        "\n",
        "        with open(forward_index_file, 'w') as output_file:\n",
        "            for document, words in self.indexer.forward_index.items():\n",
        "                formatted_list =[]\n",
        "                for word_id, freq in words.items():\n",
        "                    formatted_list.append(f\"{word_id}: {freq};\")\n",
        "                output_file.write(f\"{document}\\t{' '.join(formatted_list)}\\n\")        \n",
        "\n",
        "        print(f\"For indexing program takes {self.time_taken:.2f} seconds.\")\n",
        "\n",
        "        total_index_size = sys.getsizeof(self.indexer.forward_index) +  sys.getsizeof(self.indexer.inverted_index)\n",
        "        print(f\"Total size of index: {total_index_size} bytes with {sys.getsizeof(self.indexer.forward_index)} for forward index and {sys.getsizeof(self.indexer.inverted_index)} for inverted index \")\n",
        "\n",
        "    def run_interface(self, stop_words, output_file, inverted_index_file):\n",
        "\n",
        "        # Load forward index from output file\n",
        "        forward_index = {}\n",
        "        with open(output_file, 'r') as f:\n",
        "            for line in f:\n",
        "                word, word_id = line.strip().split()\n",
        "                forward_index[word] = int(word_id)\n",
        "        \n",
        "        # Load inverted index from inverted index file\n",
        "        inverted_index = {}\n",
        "        with open(inverted_index_file, 'r') as f:\n",
        "            for line in f:\n",
        "                word_id, posting_list = line.strip().split('\\t')\n",
        "                postings = posting_list.split(';')\n",
        "                inverted_index[int(word_id)] = {}\n",
        "                for posting in postings:\n",
        "                    if posting.strip() != '':\n",
        "                        doc_id, freq = posting.strip().split(':')\n",
        "                        inverted_index[int(word_id)][int(doc_id)] = int(freq)\n",
        "            ps = PorterStemmer()\n",
        "    \n",
        "            # User input\n",
        "            user_input_word = input(\"Enter a word: \").lower()\n",
        "            \n",
        "            # Check if the word is a stop word\n",
        "            if user_input_word in stop_words:\n",
        "                print(\"The word is a stop word.\")\n",
        "                return\n",
        "            \n",
        "            # Stem the word\n",
        "            stemmed_word = ps.stem(user_input_word)\n",
        "            \n",
        "            # Check if the stemmed word exists in the forward index\n",
        "            if stemmed_word not in forward_index:\n",
        "                print(\"The word does not exist in the index.\")\n",
        "                return\n",
        "            \n",
        "            # Get word ID\n",
        "            word_id = forward_index[stemmed_word]\n",
        "            \n",
        "            # Check if the word ID exists in the inverted index\n",
        "            if word_id not in inverted_index:\n",
        "                print(\"The word ID does not exist in the inverted index.\")\n",
        "                return\n",
        "            \n",
        "            # Print posting list information\n",
        "            \n",
        "            formatted_list=[]\n",
        "            for doc_id, freq in inverted_index[word_id].items():\n",
        "                formatted_list.append(f\"{doc_id}: {freq};\")\n",
        "            print(f\"Posting list information for word is \\n{user_input_word}: {' '.join(formatted_list)}\")\n",
        "\n",
        "class Tokenizer:\n",
        "    # This is initialization for the Tokenizer class that I have created\n",
        "    def __init__(self, stop_words=None):\n",
        "        # Here I use this to get all the stop words that we have into a set, if there are no stop words I simply initialize it as an empty set\n",
        "        self.stop_words = set(stop_words) if stop_words else set()\n",
        "\n",
        "    # This is tokenize function, eliminates numbers, converts to lowercase, and extracts word tokens from the input document to tokenize it. After eliminating stop words, it provides the final list of tokens.\n",
        "    def tokenize(self, document):\n",
        "\n",
        "        # All characters are converted to lowercase using the document.lower() function, which makes the matching case-insensitive.\n",
        "        tokens = re.findall(r'\\b\\w+\\b', document.lower()) \n",
        "        \n",
        "        # If the token has no characters that are numbers, it is included. Tokens with numbers on them are filtered away as a result.\n",
        "        #If the token is not included in the self.stop_words set of stop words, it is included.\n",
        "        # this hadles the non-aplhanumeric split on punctuations too.\n",
        "        tokens = [token for token in tokens if not any(char.isdigit() for char in token) and token not in self.stop_words] \n",
        "        \n",
        "        return tokens\n",
        "\n",
        "class WordDictionary:\n",
        "    # This is simple initializing function for Word Dictionary\n",
        "    def __init__(self):\n",
        "        # Initializing a mapping of document to word_tokens, id-to-word_token, and word_token-to-id dictionaries for the class.\n",
        "        self.word_map_id = {}\n",
        "        self.id_map_word = {}\n",
        "        self.doc_word_mapping = {}\n",
        "        # We are using existing PorterStemmer function from nlkt to stem words to their roots\n",
        "        self.stemmer = PorterStemmer()\n",
        "\n",
        "    # This function Stems a given word using the Porter Stemmer.\n",
        "    def stem(self, word):\n",
        "        return self.stemmer.stem(word)\n",
        "\n",
        "    # This function adds a new word/token to the dictionary and, if it doesn't already exist, gives it a unique numerical ID.\n",
        "    def map_word_to_id(self, word_token):\n",
        "        if word_token not in self.word_map_id:\n",
        "            word_token_id = len(self.word_map_id) + 1\n",
        "            self.word_map_id[word_token] = word_token_id\n",
        "            self.id_map_word[word_token_id] = word_token\n",
        "        else:\n",
        "            word_token_id = self.word_map_id[word_token]\n",
        "        return word_token_id\n",
        "\n",
        "    # This function adds a word ID to the document's mapping, to keep the mapping information\n",
        "    def map_word_id_to_doc(self, word_token_id, file_doc_id):\n",
        "        if file_doc_id not in self.doc_word_mapping:\n",
        "            self.doc_word_mapping[file_doc_id] = set()\n",
        "        self.doc_word_mapping[file_doc_id].add(word_token_id)\n",
        "\n",
        "class FileDictionary:\n",
        "    # Initializing a dictionary to map document IDs to internal document IDs, dictionary to map internal document IDs to the original document IDs and,\n",
        "    # a counter that keeps track of the current internal document ID\n",
        "    def __init__(self):\n",
        "        self.doc_map_id = {}\n",
        "        self.id_map_doc = {}\n",
        "        self.counter = 0\n",
        "\n",
        "    # This function associates the provided file_doc_id with the current internal document ID and vice versa.\n",
        "    def map_document_to_id(self, file_doc_id):  \n",
        "        self.counter += 1\n",
        "        self.doc_map_id[file_doc_id] = self.counter\n",
        "        self.id_map_doc[self.counter] = file_doc_id\n",
        "        return self.counter\n",
        "        \n",
        "class Indexer:\n",
        "    # This is simple initializing function for Indexers\n",
        "    def __init__(self):\n",
        "        # Initializing \n",
        "        self.forward_index = {}\n",
        "        self.inverted_index = {}\n",
        "\n",
        "\n",
        "    def index_building(self, file_doc_id, token_stemmed, word_dict_new):\n",
        "    \n",
        "    # This gets the word frequency in the document\n",
        "        word_frequency = {}\n",
        "        for word in token_stemmed:\n",
        "            if word in word_frequency:\n",
        "                word_frequency[word] += 1\n",
        "            else:\n",
        "                word_frequency[word] = 1\n",
        "        \n",
        "        \n",
        "        word_frequency_updated = {}\n",
        "\n",
        "        # Update inverted index in wordID1: docId1: freq in docID1; docId2: freq in docID2;  format\n",
        "        for word, freq in word_frequency.items():\n",
        "            if word_dict_new[word] not in self.inverted_index:\n",
        "                self.inverted_index[word_dict_new[word]] = {}\n",
        "            self.inverted_index[word_dict_new[word]][file_doc_id] = freq\n",
        "            word_frequency_updated[word_dict_new[word]] = freq\n",
        "        \n",
        "        # Update inverted index in docID1: â€¦; wordIdi: freq in docID1; wordIdi+1: freq in docID1;  format\n",
        "        self.forward_index[file_doc_id] = word_frequency_updated\n",
        "     \n",
        "\n",
        "class Query_Processing:\n",
        "    def __init__(self, stop_words=None):\n",
        "        # first we intitialize stop words using Tokenizer class\n",
        "        self.tokenizer = Tokenizer(stop_words)\n",
        "        # We create instances for Word Dictionary class\n",
        "        self.word_dict = WordDictionary()\n",
        "        # We create instances for file Dictionary class\n",
        "        self.file_dict = FileDictionary()\n",
        "        # We create instances for forward and inverted index class\n",
        "        self.indexer = Indexer()\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_tf_idf_weights(output_file, forward_index_file, inverted_index_file, queries):\n",
        "        \n",
        "        tf_idf_weights = defaultdict(dict)  # Dictionary to store TF-IDF weights for terms\n",
        "        tf_idf_query = defaultdict(dict)\n",
        "        idf_weights = {}\n",
        "        N = 0  # Total number of documents in the collection\n",
        "\n",
        "        # Read forward index file\n",
        "        with open(forward_index_file, 'r') as f:\n",
        "            forward_index_lines = f.readlines()\n",
        "\n",
        "        # Read inverted index file\n",
        "        with open(inverted_index_file, 'r') as f:\n",
        "            inverted_index_lines = f.readlines()\n",
        "\n",
        "        # Read inverted index file\n",
        "        with open(output_file, 'r') as f:\n",
        "            output = f.readlines()\n",
        "        words = output[:len(inverted_index_lines)]\n",
        "        files = output[len(inverted_index_lines):]\n",
        "        # Initialize an empty dictionary\n",
        "        word_count_dict = {}\n",
        "        word_to_num_dict = {}\n",
        "        for line in words:\n",
        "            parts = line.split('\\t')\n",
        "            word = parts[0]\n",
        "            count = int(parts[1])  \n",
        "            word_count_dict[count] = word\n",
        "            word_to_num_dict[word] = count\n",
        "\n",
        "        file_count_dict = {}\n",
        "        for line in files:\n",
        "            parts = line.split('\\t')\n",
        "            file = parts[0]\n",
        "            num = int(parts[1])  \n",
        "            file_count_dict[num] = file\n",
        "\n",
        "\n",
        "        N = len(forward_index_lines)\n",
        "\n",
        "        # Calculate IDF for each term in the collection based on the inverted index\n",
        "        for line in inverted_index_lines:\n",
        "            term_id, postings = line.strip().split('\\t')\n",
        "            df = len(postings.rstrip(';').split(';'))  # Document frequency (df) of the term\n",
        "            idf = math.log(N / df) if df > 0 else 0  # IDF calculation\n",
        "\n",
        "            idf_weights[int(term_id)] = idf\n",
        "        \n",
        "        for line in forward_index_lines:    \n",
        "            file_id, term = line.strip().split('\\t')\n",
        "            term_frequency = term.rstrip(';').split(';')\n",
        "            for item in term_frequency:\n",
        "                term_id, tf = item.rstrip(';').split(':')\n",
        "                tf_idf_weights[word_count_dict.get(int(term_id), 0)][file_count_dict.get(int(file_id), 0)] = int(tf) * idf_weights.get(int(term_id), 0)\n",
        "\n",
        "        tf_idf_table = pd.DataFrame(tf_idf_weights).T\n",
        "\n",
        "\n",
        "        for key, item in queries.items():\n",
        "            for i, j in item.items():\n",
        "                for k in set(j):\n",
        "                    if str(k) in tf_idf_weights: \n",
        "                        term_id = word_to_num_dict.get(str(k), 0)\n",
        "                        tf_idf_weights[str(k)][f\"{key}_{i}\"] = int(j.count(k)) * idf_weights.get(int(term_id), 0)\n",
        "                    else:\n",
        "                        tf_idf_weights[str(k)] = {}\n",
        "                        term_id = word_to_num_dict.get(str(k), 0)\n",
        "                        tf_idf_weights[str(k)][f\"{key}_{i}\"] = int(j.count(k)) * idf_weights.get(int(term_id), 0)\n",
        "\n",
        "        tf_idf_query = pd.DataFrame(tf_idf_weights).T\n",
        "\n",
        "\n",
        "        return tf_idf_query[tf_idf_table.columns] , tf_idf_query.drop(tf_idf_table.columns, axis=1)\n",
        "\n",
        "    \n",
        "    @staticmethod\n",
        "    def parse_topics_file(topics_file, stop_words):\n",
        "        extracted_values = {}\n",
        "        num_pattern = r'<num> Number:\\s*(\\d+)'\n",
        "        title_pattern = r'<title>\\s*(.*?)\\s*<desc>'\n",
        "        desc_pattern = r'<desc> Description:\\s*(.*?)\\s*<narr>'\n",
        "        narr_pattern = r'<narr> Narrative:\\s*(.*?)\\s*$'\n",
        "        with open(topics_file, 'r') as file:\n",
        "            file_content = file.read()\n",
        "        \n",
        "        doc_tag_matches = re.finditer(r'<top>(.*?)<\\/top>', file_content, re.DOTALL)\n",
        "        \n",
        "        for idx, doc_tag_match in enumerate(doc_tag_matches, 1):\n",
        "            # get the content between \"<top>...</top>\" \n",
        "            document_content = doc_tag_match.group(1)\n",
        "            doc_values = {}\n",
        "            ps = PorterStemmer()\n",
        "\n",
        "            # Extract document number\n",
        "            num_match = re.search(num_pattern, document_content)\n",
        "            num = num_match.group(1) if num_match else \"\"\n",
        "\n",
        "            # Extract title\n",
        "            title_match = re.search(title_pattern, document_content, re.DOTALL)\n",
        "            t = title_match.group(1).strip() if title_match else \"\"\n",
        "            title = re.findall(r'\\b\\w+\\b', t.lower()) \n",
        "            title =  [token for token in title if not any(char.isdigit() for char in token) and token not in stop_words] \n",
        "            main_title = [ps.stem(word) for word in title]\n",
        "\n",
        "            # Extract description\n",
        "            desc_match = re.search(desc_pattern, document_content, re.DOTALL)\n",
        "            d = desc_match.group(1).strip().replace('\\n', '') if desc_match else \"\"\n",
        "            description = re.findall(r'\\b\\w+\\b', d.lower()) \n",
        "            description =  [token for token in description if not any(char.isdigit() for char in token) and token not in stop_words] \n",
        "            main_description =  [ps.stem(word) for word in description]\n",
        "            \n",
        "            # Extract narrative\n",
        "            narr_match = re.search(narr_pattern, document_content, re.DOTALL)\n",
        "            n = narr_match.group(1).strip() if narr_match else \"\"\n",
        "            narrative = re.findall(r'\\b\\w+\\b', n.lower())\n",
        "            narrative =  [token for token in narrative if not any(char.isdigit() for char in token) and token not in stop_words] \n",
        "            main_narration = [ps.stem(word) for word in narrative]\n",
        "\n",
        "            doc_values['title'] = main_title\n",
        "            doc_values['description_title'] = main_title + main_description\n",
        "            doc_values['narrative_title'] =  main_title + main_narration\n",
        "\n",
        "            # Store extracted values with document ID as key\n",
        "            extracted_values[num] = doc_values\n",
        "\n",
        "        return extracted_values\n",
        "    \n",
        "    @staticmethod\n",
        "    def compare_queries_to_documents(tf_idf_query, tf_idf_table, output_file_path, relevancy_path):\n",
        "        similarities = defaultdict(dict)\n",
        "         \n",
        "        sorted_similarities= defaultdict(dict)\n",
        "        # Iterate through each query column\n",
        "        for query_column in tf_idf_query.columns:\n",
        "            query_vector = tf_idf_query[query_column].values\n",
        "            \n",
        "            unique_counter = 0\n",
        "            \n",
        "            # Dictionary to store document similarities for each query\n",
        "            document_similarities = {}\n",
        "            \n",
        "            # Iterate through each document column\n",
        "            for document_column in tf_idf_table.columns:\n",
        "                document_vector = tf_idf_table[document_column].values\n",
        "                \n",
        "                # Calculate cosine similarity between query and document vectors\n",
        "                cosine_similarity_score = Query_Processing.cosine_similarity(query_vector, document_vector)\n",
        "\n",
        "                similarities[query_column][document_column] = cosine_similarity_score\n",
        "\n",
        "\n",
        "            for query, documents in similarities.items():\n",
        "                # Sort documents based on cosine similarity score\n",
        "                sorted_documents = dict(sorted(documents.items(), key=itemgetter(1), reverse=True))\n",
        "                sorted_similarities[query] = sorted_documents\n",
        "\n",
        "            file_path = \"vsm_output_all.txt\"\n",
        "            \n",
        "            # Open the files in write mode\n",
        "            with open(file_path, 'w') as file, open(relevancy_path, 'w') as rel, open(output_file_path, 'w') as out:\n",
        "                file.write(\"TOPIC\\tDOCUMENT\\tUNIQUE#\\tCOSINE_VALUE\\n\")\n",
        "                rel.write(\"TOPIC\\tITERATION\\tDOCUMENT\\tRELEVANCY\\n\")\n",
        "                out.write(\"TOPIC\\tDOCUMENT\\tUNIQUE#\\tCOSINE_VALUE\\n\")\n",
        "\n",
        "                # Iterate over each topic and its documents\n",
        "                for topic, documents in sorted_similarities.items():\n",
        "                    UNIQUE = 0\n",
        "                    for document, cosine_value in documents.items():\n",
        "                        # Write data to file in the specified format\n",
        "                        UNIQUE += 1\n",
        "                        file.write(f\"{topic}\\t{document}\\t{UNIQUE}\\t{cosine_value}\\n\")\n",
        "                        if cosine_value > 0:\n",
        "                            relevancy = 1\n",
        "                        else:\n",
        "                            relevancy = 0\n",
        "                        rel.write(f\"{topic}\\t0\\t{document}\\t{relevancy}\\n\")\n",
        "\n",
        "                        if \"_title\" in topic and \"_description\" not in topic and \"_narrative\" not in topic:\n",
        "                            out.write(f\"{''.join(filter(str.isdigit, topic))}\\t{document}\\t{UNIQUE}\\t{cosine_value}\\n\")\n",
        "                \n",
        "        return sorted_similarities\n",
        "\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def cosine_similarity(query_vector, document_vectors):\n",
        "        # Calculate dot product between query vector and document vectors\n",
        "        query_vector = np.nan_to_num(query_vector)\n",
        "        document_vectors = np.nan_to_num(document_vectors)\n",
        "        dot_products = np.dot(document_vectors, query_vector)\n",
        "        \n",
        "        # Calculate magnitudes of query vector and document vectors\n",
        "        query_magnitude = np.linalg.norm(query_vector)\n",
        "        document_magnitudes = np.linalg.norm(document_vectors)\n",
        "        \n",
        "        # Avoid division by zero\n",
        "        if document_magnitudes == 0 or query_magnitude==0:\n",
        "            document_magnitudes=1\n",
        "            query_magnitude = 1\n",
        "        \n",
        "        # Calculate cosine similarity scores\n",
        "        cosine_similarities = dot_products / (query_magnitude * document_magnitudes)\n",
        "        \n",
        "        return cosine_similarities\n",
        "\n",
        "    @staticmethod\n",
        "    def relevancy(mqrels_path, relevancy_path):\n",
        "            mqrels = {}\n",
        "            with open(mqrels_path, 'r') as mqrels_file:\n",
        "                for line in mqrels_file:\n",
        "                    topic, _, document, relevance = line.strip().split()\n",
        "                    mqrels.setdefault(topic, {}).setdefault(document, int(relevance))\n",
        "\n",
        "            # Read relevancy file\n",
        "            relevancy = {}\n",
        "            with open(relevancy_path, 'r') as relevancy_file:\n",
        "                next(relevancy_file)\n",
        "                for line in relevancy_file:\n",
        "                    topic, _, document, relevance = line.strip().split()\n",
        "                    relevancy.setdefault(topic, {}).setdefault(document, int(relevance))\n",
        "\n",
        "            # Compute precision and recall for each topic and document\n",
        "            results = {}\n",
        "            for topic_o in relevancy.keys():\n",
        "                tp = fp = fn = 0\n",
        "                for document, rel in relevancy[topic_o].items():\n",
        "                    topic = ''.join(filter(str.isdigit, topic_o)) \n",
        "                    if document in mqrels.get(topic, {}):\n",
        "                        mqrels_relevance = mqrels[topic][document]\n",
        "                        relevance = int(rel)\n",
        "                        if mqrels_relevance == relevance:\n",
        "                            tp += 1\n",
        "                        elif mqrels_relevance == 1 and relevance == 0:\n",
        "                            fn += 1\n",
        "                        elif mqrels_relevance == 0 and relevance == 1:\n",
        "                            fp += 1\n",
        "                precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "                recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "                results[topic_o] = {'precision': precision, 'recall': recall}\n",
        "                \n",
        "            return results\n",
        "\n",
        "\n",
        "# The actual code starts running from here. This is the main function\n",
        "def main(stop_file, input_file, output_file, forward_index_file, inverted_index_file , testing, interface, start_time,query_processing,qrels_file):\n",
        "    \n",
        "\n",
        "    # Initially I am simply load all the stopwords from the given file. \n",
        "    with open(stop_file, 'r') as file:\n",
        "        stop_words = set(line.strip() for line in file.read().splitlines())\n",
        "\n",
        "    # creating an instance for the Text parser.\n",
        "    parser_instance = TextParser_Indexer(stop_words=stop_words)\n",
        "\n",
        "    # loading all files with given input file path + _i here i is all numbers from 1 to 15.\n",
        "    if testing == \"Y\" :\n",
        "\n",
        "        parser_instance.parse_file(input_file, forward_index_file)\n",
        "        \n",
        "        # Saving the extra output of word and file dictionaries for all these files\n",
        "        parser_instance.save_dictionary(output_file, inverted_index_file, forward_index_file)\n",
        "\n",
        "    elif not os.path.exists(output_file):\n",
        "        files = [f\"_{i}\" for i in range(1, 16)]\n",
        "\n",
        "        # For each file, I parse them individually one after the another. Here I am simply passing file names to parse_file function\n",
        "        for file in files:\n",
        "            parser_instance.parse_file(input_file + file, forward_index_file)\n",
        "\n",
        "        # Saving the extra output of word and file dictionaries for all these files\n",
        "        parser_instance.save_dictionary(output_file, inverted_index_file, forward_index_file)\n",
        "\n",
        "    # Example usage:\n",
        "    if query_processing == \"Y\" :\n",
        "        relevancy =\"N\"\n",
        "        input_file = \"C:/Users/badda/Downloads/proj1/topics.txt\"\n",
        "        output_file_path = 'vsm_output.txt'\n",
        "        relevancy_path = 'relevancy.txt'\n",
        "        mqrels_path = \"main.qrels\"\n",
        "        results_path = \"results.txt\"\n",
        "\n",
        "        query_instance = Query_Processing(stop_words=stop_words)\n",
        "        \n",
        "        if relevancy == \"N\":\n",
        "            queries = query_instance.parse_topics_file(input_file, stop_words)\n",
        "            tf_idf_table,tf_idf_query = query_instance.calculate_tf_idf_weights(output_file, forward_index_file, inverted_index_file, queries)\n",
        "            similarities = query_instance.compare_queries_to_documents(tf_idf_query, tf_idf_table, output_file_path, relevancy_path)\n",
        "        \n",
        "        results = query_instance.relevancy(mqrels_path, relevancy_path)\n",
        "\n",
        "        with open(results_path, 'w') as results_file:\n",
        "            # Iterate over each entry in the dictionary\n",
        "            for topic, metrics in results.items():\n",
        "                # Write topic name\n",
        "                results_file.write(f\"Topic: {topic}\\n\")\n",
        "                # Write precision and recall values\n",
        "                results_file.write(f\"Precision: {metrics['precision']}\\n\")\n",
        "                results_file.write(f\"Recall: {metrics['recall']}\\n\\n\")\n",
        "\n",
        "    print(f\"The total time taken to run the entire program including Tokenization and Indexing is {(time.time() - start_time)} seconds\")\n",
        "\n",
        "    if interface == \"Y\":\n",
        "        parser_instance.run_interface(stop_words, output_file, inverted_index_file)\n",
        "       \n",
        "\n",
        "\n",
        "#This runs the whole code. \n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # set this flag if you want to run test file\n",
        "    testing = \"N\"\n",
        "    interface = \"N\"\n",
        "    query_processing =\"Y\"\n",
        "\n",
        "    # input variables\n",
        "    input_file = \"C:/Users/badda/Downloads/proj1/ft911/ft911\"\n",
        "    stop_file = \"stopwordlist.txt\"\n",
        "    output_file = \"parser_output.txt\" \n",
        "    forward_index_file = \"forward_index.txt\" \n",
        "    inverted_index_file = \"inverted_index.txt\" \n",
        "    qrels_file = \"main.qrels\"\n",
        "\n",
        "    #run main for TREC data \n",
        "    if testing != \"Y\":\n",
        "        \n",
        "        main(stop_file, input_file, output_file, forward_index_file, inverted_index_file , testing, interface, start_time, query_processing,qrels_file)\n",
        "        \n",
        "    elif testing == \"Y\":\n",
        "        input_file = \"C:/Users/badda/Downloads/proj1/topics.txt\"\n",
        "        main(stop_file, input_file, f\"test_{output_file}\", f\"test_{forward_index_file}\" , f\"test_{inverted_index_file}\", testing, interface, start_time, query_processing,qrels_file)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Topic: 352_title\n",
        "# Precision: 0.2777777777777778\n",
        "# Recall: 1.0\n",
        "\n",
        "# Topic: 352_description_title\n",
        "# Precision: 0.2222222222222222\n",
        "# Recall: 1.0\n",
        "\n",
        "# Topic: 352_narrative_title\n",
        "# Precision: 0.08333333333333333\n",
        "# Recall: 1.0\n",
        "\n",
        "# Topic: 353_narrative_title\n",
        "# Precision: 0.4782608695652174\n",
        "# Recall: 1.0\n",
        "\n",
        "# Topic: 353_description_title\n",
        "# Precision: 0.7142857142857143\n",
        "# Recall: 0.8823529411764706\n",
        "\n",
        "# Topic: 354_description_title\n",
        "# Precision: 0.34615384615384615\n",
        "# Recall: 0.9\n",
        "\n",
        "# Topic: 354_narrative_title\n",
        "# Precision: 0.34615384615384615\n",
        "# Recall: 0.9\n",
        "\n",
        "# Topic: 359_narrative_title\n",
        "# Precision: 0.07692307692307693\n",
        "# Recall: 1.0\n",
        "\n",
        "# Topic: 359_title\n",
        "# Precision: 0.07692307692307693\n",
        "# Recall: 1.0\n",
        "\n",
        "# Topic: 359_description_title\n",
        "# Precision: 0.07692307692307693\n",
        "# Recall: 1.0\n",
        "\n",
        "# Topic: 353_title\n",
        "# Precision: 0.9\n",
        "# Recall: 0.8571428571428571\n",
        "\n",
        "# Topic: 354_title\n",
        "# Precision: 0.75\n",
        "# Recall: 0.8571428571428571\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
